{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning-Value functions, Action-State functions, DQN and all that\n",
    "\n",
    "Reinforcement learning (RL) is a task where we seek to find a machine learning algorithm that can sample/explore and interact with an enviornment in way that encourages actions towards a goal.  This is different from most machine learning tasks because in an RL problem, there is no labeled data set, in fact there is no data at all! This particular field of ML requires the algorithm to both generate data and exploit its generated data in order to drive the choices it makes towards some defined goal.  Naturally, with so little to start on in this problem we're going to need to formalize this problem and to this end RL is best described by Markov reward and Markov Decision processes (MRP/MDP).  In the following text we will touch on the mathematics of an MRP/MDP as well as demonstrating each step via some simple python scripts.  We will explore how an agent can influence an MDP and that the task of RL is seek the optimal influence in an MDP to accomplish a goal.  Ultimately we will be driving towards using neural networks as a form of approximate solution for optimal influence in any MDP and we will learn why that is nessecary.  The work will conclude with generating a neural network which will learn to balance an upright pendulum on a cart which has freedom to accelerate left or right in a single plane, amazingly we will not give the nueral network any information about physics to accomplish this task.  The outline of the work is given below.  \n",
    "\n",
    "+ Markov Reward Process\n",
    "+ Compact Mapping Theorem - Iterative Solution Details\n",
    "+ Markov Decision process\n",
    "+ Seeking the optimal policy\n",
    "    - General Value Policy Improvement loop\n",
    "\n",
    "+ Unknown transition probability MDP\n",
    "+ Temporal difference learning\n",
    "+ Q-learning\n",
    "+ Large state spaces and approximate Q-learning\n",
    "    - How Does One Solve For $\\theta$?\n",
    "    - Experience Replay and Target Networks\n",
    "+ Conclusion\n",
    "\n",
    "## 1. Markov Reward Process \n",
    "\n",
    "Lets start with the simplest version of a MDP, ie the Markov reward process (MRP). the MRP is simple Markov chain sequence, where we include rewards for each state transition. In a MRP there is no agency/influence for step to step transitions during the process.  Formally a Markov chain consists of a sequence of states $S=\\{s_0,s_1,s_2,...,s_N\\}$ which are sampled from a Markov process defined by a transition probability $P(s'|s)$ which gives the transition probability from state $s \\rightarrow s'$. In a MRP, after each state $s$ to state $s'$ transition a reward dependent on the transition $r_{s',s}$ is sampled from $P(s',r|s)$.  Formally any sample sequence from an MRP has the form $S=\\{(s_0,s_1,r_1),(s_1,s_2,r_2),...,(s_{N-1},s_N,r_N)\\}$. For a particular sample sequence of the MRP the goal/long term reward of this sequence is computed after step $t$ by\n",
    "\n",
    "$G({s_t,s_{t+1},s_{t+2},...,s_{T}}) = G_t = r_{t+1} + \\gamma \\ r_{t+2} + ...+\\gamma^N \\ r_{t+N} = \\sum_{n=1}^{N} \\gamma^n r_{t+n}$\n",
    "\n",
    "Where $t+N=T$ is the terminal state of the sequence. We will denote any terminal state with $t=T$ from here on. The value of $t$ can be the initial step $t=0$ or any value in the sequence up to the terminal state step $t=T$, so that we can evaluate goals many times from one sequence. The long term reward also contains a parameter $\\gamma$, called the discount factor, taken to be $0 \\leq \\gamma \\leq 1$.  This parameter lets us control what is considered more import, immediate (state to state) rewards $\\gamma \\rightarrow 0$ or longer term over a whole sequence rewards $\\gamma \\rightarrow 1$.  With long term reward defined we can also define a value function $v(s)$ defined as the exception over all possible sequences of the MRP for the long term reward from any state $s$ onward to any terminal states.  This function is useful because it allows us to know on average how good is it to be in a state $s$ in terms of long term rewards.  We can compute this value on an MRP via \n",
    "\n",
    "$v(s) = \\mathbb{E}\\left[ G_t | s \\right] = \\sum_{s',r}P(s',r|s)\\left(r +\\gamma \\sum_{s'',r'} P(s'',r'|s')(r' + \\gamma \\sum_{s''',r''}(...)\\right)$\n",
    "\n",
    "which can be expressed by the recursive function\n",
    "\n",
    "$v(s) = \\sum_{s',r} P(s',r|s)\\left( r + \\gamma \\ v(s') \\right)$\n",
    "\n",
    "and simplified by defining the average instantaneous reward (reward given after each state transition) as\n",
    "\n",
    "$r(s) = \\sum_{s',r} P(s',r|s)r$\n",
    "\n",
    "giving\n",
    "\n",
    "$v(s) = r(s) + \\gamma \\sum_{s',r} P(s',r|s) v(s')$\n",
    "\n",
    "This type of definition, with a finite number of states (values $s$ can take) allows us to express this relationship in terms of tensors ${\\bf P}_{s,r,s'} = P(s',r|s)$ and ${\\bf v}_s = v(s)$, ${\\bf r}_s = r(s)$.  In cases where only one reward is possible from $s\\rightarrow s'$ transitions, the transition tensor collapses to a standard matrix and we can solve the value function via simple matrix inversion\n",
    "\n",
    "${\\bf v} = ({\\bf 1}-\\gamma {\\bf P})^{-1} {\\bf r}$\n",
    "\n",
    "Lets take a look at an example simple MRP and apply this inversion below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v(state)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bar</th>\n",
       "      <td>1.908392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pass_class</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_2</th>\n",
       "      <td>0.942655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_3</th>\n",
       "      <td>4.087021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_1</th>\n",
       "      <td>-5.012729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook</th>\n",
       "      <td>-7.637608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sleep</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             v(state)\n",
       "state                \n",
       "bar          1.908392\n",
       "pass_class  10.000000\n",
       "class_2      0.942655\n",
       "class_3      4.087021\n",
       "class_1     -5.012729\n",
       "facebook    -7.637608\n",
       "sleep        0.000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Markov Reward Process\n",
    "# solving the value function v(s) \n",
    "# when P(s',r|s) is known\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# this example is derived from David Silvers RL class\n",
    "\n",
    "# Our process simulates the sequnce of attending 3 classes and passing the class\n",
    "# where you then reach the terminal state sleep.  Some pitfalls are going on facebook\n",
    "# or going to the bar which have non-zero probablities of returning to classes or \n",
    "# continuing on facebook.\n",
    "cols = np.array(['class_1','class_2','class_3','pass_class','bar','facebook','sleep'])\n",
    "\n",
    "# in the simplest cases P(s,r|s') decomposes to P(s'|s)R(s'|s)\n",
    "\n",
    "# state to state transition probablity matrix P(s'|s)\n",
    "# the rows are the current state s, and the columns\n",
    "# the potential transition state s'\n",
    "P=np.array([[0.0,0.5,0.0,0.0,0.0,0.5,0.0],\n",
    "            [0.0,0.0,0.8,0.0,0.0,0.0,0.2],\n",
    "            [0.0,0.0,0.0,0.6,0.4,0.0,0.0],\n",
    "            [0.0,0.0,0.0,0.0,0.0,0.0,1.0],\n",
    "            [0.2,0.4,0.4,0.0,0.0,0.0,0.0],\n",
    "            [0.1,0.0,0.0,0.0,0.0,0.9,0.0],\n",
    "            [0.0,0.0,0.0,0.0,0.0,0.0,1.0]])\n",
    "\n",
    "# state to state reward matrix R(s'|s)\n",
    "# passing the class and going to sleep is given\n",
    "# +10 points all other actions are given negative\n",
    "# rewards except going to the bar which has a minor\n",
    "# +1 reward\n",
    "R=np.array([[0.0,-2,0.0,0.0,0.0,-2.0,0.0],\n",
    "            [0.0,0.0,-2.,0.0,0.0,0.0,-2.],\n",
    "            [0.0,0.0,0.0,-2.,-2.,0.0,0.0],\n",
    "            [0.0,0.0,0.0,0.0,0.0,0.0,10.],\n",
    "            [1.0,1.0,1.0,0.0,0.0,0.0,0.0],\n",
    "            [-1.,0.0,0.0,0.0,0.0,-1.,0.0],\n",
    "            [0.0,0.0,0.0,0.0,0.0,0.0,0.0]])\n",
    "\n",
    "# mean instanetous reward r(s)\n",
    "r = np.matmul(P,R.T).diagonal()\n",
    "\n",
    "# gamma value\n",
    "y = 0.9\n",
    "\n",
    "# solving the value function v(s) via matrix inversion\n",
    "v = np.matmul(np.linalg.inv(np.eye(7)-y*P), r)\n",
    "\n",
    "# print the value functions with their labels\n",
    "valuefn_dict = dict(zip(cols,v))\n",
    "\n",
    "# using pandas allows us to have a pretty output\n",
    "value_df = pd.DataFrame(valuefn_dict.items(),columns=['state','v(state)'])\n",
    "value_df.set_index('state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously being in the bar is good for the short term, but being in class 3 is much better as it is likely we will have the chance to pass the class.  Being in class 1 isn't so great as it has high likelyhood of moving to facebook which will not help us reach our ultimate goal.  Naturally this leads us towards the ability to make decisions on an MRP, ie infleunce which state to state transitions one takes, but before we do that lets discuss some methods of solving for a value function that do not involve matrix inversion.\n",
    "\n",
    "\n",
    "## 2. Compact Mapping Theorem-Iterative Solution Details\n",
    "\n",
    "Unfortunately the inversion of the matrices above can become a problem for other algorithms we will encounter so rather than inverting a matrix, we will use fixed point methods/iterative solutions.  We can use these methods when we satisfy the compact mapping theorem (CMT).  The CMT can be described below by first defining the Euclidean norm as\n",
    "\n",
    "$||{\\bf u}||=||u(s)|| = \\sqrt{ \\sum_s u(s)^2 }$\n",
    "\n",
    "and we define a compact mapping function $T[{\\bf v}]$ to be that of our value function condition\n",
    " \n",
    "$T[v(s)] = r(s)+\\gamma \\sum_{s',r}P(s',r|s) v(s')$\n",
    "\n",
    "If we look at two different vectors and how they are mapped to new vectors under $T$ we find\n",
    "\n",
    "$||T(v)-T(u)|| = \\gamma \\sum_{s',r} ||P(s',r|s) (v(s')-u(s'))|| \\leq \\gamma ||{\\bf v}-{\\bf u}||$\n",
    "\n",
    "Therefore if we define iterative values of the vector ${\\bf v}$ as\n",
    "\n",
    "$v^{k+1}(s) = T[ v^k(s) ]$\n",
    "\n",
    "then after $k$ applications of the mapping function we have\n",
    "\n",
    "$||T(v^k) - T(v^{k-1})|| \\leq \\gamma^k || {\\bf v}^1 - {\\bf v}^0||$\n",
    "\n",
    "and because $|| {\\bf v}^1 - {\\bf v}^0||$ is finite and positive then as $k \\rightarrow \\infty$\n",
    "\n",
    "we have\n",
    "\n",
    "$T(v^\\infty) = v^\\infty$\n",
    "\n",
    "Therefore the solution $v(s)$ we seek will be the fixed point of this process.  I demonstrate this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('converged -> steps: ', 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v(state)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bar</th>\n",
       "      <td>1.912307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pass_class</th>\n",
       "      <td>10.003301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_2</th>\n",
       "      <td>0.946171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_3</th>\n",
       "      <td>4.090579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_1</th>\n",
       "      <td>-5.007441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook</th>\n",
       "      <td>-7.630726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sleep</th>\n",
       "      <td>0.003301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             v(state)\n",
       "state                \n",
       "bar          1.912307\n",
       "pass_class  10.003301\n",
       "class_2      0.946171\n",
       "class_3      4.090579\n",
       "class_1     -5.007441\n",
       "facebook    -7.630726\n",
       "sleep        0.003301"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the previous problem but using iterative solving method\n",
    "\n",
    "# random initialization of the value function\n",
    "v = np.random.randn(7)\n",
    "\n",
    "# iterate until convergence as measured\n",
    "# by delta\n",
    "converge_delta = 0.001\n",
    "\n",
    "# if it doesnt converge by the delta standard\n",
    "# after MAX_ITERATIONS stop there\n",
    "MAX_ITERATIONS = 1000\n",
    "iterator = 0\n",
    "\n",
    "# test for convergence with this\n",
    "v_old = v \n",
    "\n",
    "continue_flag = True\n",
    "while continue_flag or iterator == MAX_ITERATIONS:\n",
    "    v = r + y*np.matmul(P,v)\n",
    "    diff = (v - v_old)**2;\n",
    "    # convergence checking condition\n",
    "    if (not np.any(diff > converge_delta**2)):\n",
    "        continue_flag = False\n",
    "        print('converged -> steps: ',iterator)\n",
    "    \n",
    "    v_old = v\n",
    "    iterator += 1\n",
    "    \n",
    "# print the value functions with their labels\n",
    "valuefn_dict = dict(zip(cols,v))\n",
    "\n",
    "# using pandas allows us to have a pretty output\n",
    "value_df = pd.DataFrame(valuefn_dict.items(),columns=['state','v(state)'])\n",
    "value_df.set_index('state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Markov Decision Process\n",
    "\n",
    "In a Markov decision process (MDP) we insert the ability to influence the state to state transition probability via an additional distribution $\\pi(a|s)$ known as a policy.  This reflects the random sample of an action $a$ when in a state $s$ which impacts the state to state transition probability. We can now express its influence as\n",
    "\n",
    "$P(s',r|s) \\rightarrow P_\\pi(s',r|s) = \\sum_{a}P(s',r|s,a)\\pi(a|s)$\n",
    "\n",
    "Now we've introduced a distribution $P(s',r|s,a)$ which is the probability of transitioning from state $s$ to state $s'$ after performing action $a$ and receiving reward $r$.  With these modifications we can evaluate the value function $v(s)$ given our choice of policy function $\\pi(a|s)$ expressed as \n",
    "\n",
    "$v[s,\\pi(a|s)] = \\mathbb{E}_\\pi[ G_t| S_t = s]$\n",
    "\n",
    "$v[s,\\pi(a|s)] = \\sum_{s',r,a} P(s',r|s,a)\\pi(a|s)[ r+\\gamma v[s',\\pi(\\cdot|s')] ]$\n",
    "\n",
    "an alternative expression allows\n",
    "\n",
    "$v_{\\pi}(s) = \\sum_a \\pi(a|s) \\left(\\sum_{s',r}P(s',r|s,a)(r+\\gamma v_{\\pi}(s')\\right)$\n",
    "\n",
    "$v_{\\pi}(s) = \\sum_a \\pi(a|s) q_{\\pi}(s,a)$\n",
    "\n",
    "Where now we've introduced a $q$ factor, known as a state-action return. Expressed in terms of a mean immediate reward given an action is\n",
    "\n",
    "$r(s,a) = \\sum_{s',r}P(s',r|s,a)r$\n",
    "\n",
    "$q_{\\pi}(s,a) = r(s,a) + \\gamma \\sum_{s',r} P(s',r|s,a)v_{\\pi}(s')$\n",
    "\n",
    "which allows the recursive expressions\n",
    "\n",
    "$q_{\\pi}(s,a) = r(s,a) + \\gamma \\sum_{s',r} P(s',r|s,a)\\sum_{a'} \\pi(a'|s') q_{\\pi}(s',a')$\n",
    "\n",
    "$v_{\\pi}(s) = \\sum_a \\pi(a|s)r(s,a)+\\gamma \\sum_{s',r,a}P(s',r|s,a) \\pi(a|s) v_{\\pi}(s')$\n",
    "\n",
    "$v_{\\pi}(s) = r_{\\pi}(s)+\\gamma \\sum_{s',r}P(s',r|s) v_{\\pi}(s')$\n",
    "\n",
    "The obvious question is how can we choose $\\pi$ such that it maximizes all values of $v_\\pi(s)$ for any $s$?  To answer that we must be able to evaluate $v_\\pi(s)$ for any $\\pi$ first.  That way we can determine which policys are better than others.  We can do so using the iterative method to solve for the fixed point of the above value function equations.  I demonstrate a fixed random policy evaluation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('converged -> steps: ', 83)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v(state)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s2</th>\n",
       "      <td>26.278423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s1</th>\n",
       "      <td>-17.969924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s0</th>\n",
       "      <td>85.011930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        v(state)\n",
       "state           \n",
       "s2     26.278423\n",
       "s1    -17.969924\n",
       "s0     85.011930"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evalutating a policy v_pi(s) when a \n",
    "# random policy is used \n",
    "# and P(s',r|s,a) is known\n",
    "\n",
    "# the example below is from an MDP in A. Geron\n",
    "# Hands on Machine Learning with scikit learn\n",
    "# and Tensorflow.  This MDP is 3 state system\n",
    "# {s0,s1,s2}, where s0 has 3 available actions\n",
    "# s1 has 2 available actions and s2 has 1 action\n",
    "# available to it.  State s0 will give you a small\n",
    "# reward if you stay there, but a large reward will\n",
    "# be given if the agent can proceed to state s2 but\n",
    "# it will have to overcome a penalty of transitioning\n",
    "# from s1 to s2. NOTE THERE IS NO TERMINAL STATE HERE!\n",
    "# LUCKILY THE GAMMA BEING LESS THAN 1 ALLOWS CONVERGENCE\n",
    "\n",
    "# used to represent values that dont exist in the system\n",
    "nan = np.nan\n",
    "\n",
    "# again we decompose P(s',r|s,a)=P(s'|s,a)R(s'|s,a)\n",
    "# P(s',r|s,a) transition probablities\n",
    "P = np.array([#shape=[s,a,s']\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],\n",
    "    [[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]]\n",
    "    ])\n",
    "\n",
    "# R(s'|s,a) rewards matrix\n",
    "R = np.array([#shape=[s,a,s']\n",
    "    [[10., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n",
    "    [[0.0, 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -50.]],\n",
    "    [[nan, nan, nan], [40., 0.0, 0.0], [nan, nan, nan]]\n",
    "    ])\n",
    "\n",
    "# actions avaliable per state\n",
    "# shape=[s, action_space]\n",
    "possible_actions = [[0,1,2],[0,2],[1]]\n",
    "\n",
    "# setting the policy function pi(a|s) to random action\n",
    "#shape=[s,a]\n",
    "pi = np.full((3,3),nan)\n",
    "for s,a in enumerate(possible_actions):\n",
    "    pi[s,a] = 1./len(possible_actions[s])\n",
    "    \n",
    "# discount factor\n",
    "y = 0.9\n",
    "\n",
    "# compute the average instantous reward under policy pi\n",
    "# r(s,a) = sum_s' P(s'|s,a)R(s'|s,a) -> sum( P[s,a,s'] R[s,a,s'], s'=0,2)\n",
    "r_sa = np.array([[ np.sum((P*R)[s,a]) for a in range(3)]  for s in range(3)])\n",
    "\n",
    "# r_pi = sum_a pi(a|s) r(s,a) -> sum( pi[s,a] r_sa[s,a],a = actions(s))\n",
    "r_pi = np.array([ np.sum((pi*r_sa)[s,possible_actions[s]]) for s in range(3)])\n",
    "\n",
    "# compute the state to state transition policy under the current policy P_pi(s'|s)\n",
    "# sum_a P(s'|s,a)pi(a|s) -> sum( P[s,a,s']pi[s,a], a=actions(s))\n",
    "P_s1s = np.array([[ np.sum(P[s,possible_actions[s],s1]*pi[s,possible_actions[s]]) for s in range(3)]  for s1 in range(3)])\n",
    "\n",
    "# now we can iterate the v(s) function towards the fixed point\n",
    "# using the same code as before\n",
    "\n",
    "# random initialization of the value function\n",
    "v = np.random.randn(3)\n",
    "\n",
    "# iterate until convergence as measured\n",
    "# by delta\n",
    "converge_delta = 0.001\n",
    "\n",
    "# if it doesnt converge by the delta standard\n",
    "# after MAX_ITERATIONS stop there\n",
    "MAX_ITERATIONS = 1000\n",
    "iterator = 0\n",
    "\n",
    "# test for convergence with this\n",
    "v_old = v \n",
    "\n",
    "continue_flag = True\n",
    "while continue_flag or iterator == MAX_ITERATIONS:\n",
    "    v = r_pi + y*np.matmul(P_s1s,v)\n",
    "    diff = (v - v_old)**2;\n",
    "    # convergence checking condition\n",
    "    if (not np.any(diff > converge_delta**2)):\n",
    "        continue_flag = False\n",
    "        print('converged -> steps: ',iterator)\n",
    "    \n",
    "    v_old = v\n",
    "    iterator += 1\n",
    "    \n",
    "# print the value functions with their labels\n",
    "valuefn_dict = dict(zip(['s0','s1','s2'],v))\n",
    "\n",
    "# using pandas allows us to have a pretty output\n",
    "value_df = pd.DataFrame(valuefn_dict.items(),columns=['state','v(state)'])\n",
    "value_df.set_index('state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Seeking the Optimal Policy\n",
    "\n",
    "The optimal policy, $\\pi(a|s)$, is the function such that we maximize the value function at each state\n",
    "\n",
    "$v_*(s) = $max$_{\\pi} v[s,\\pi(a|s)]$\n",
    "\n",
    "Translating the math to english, the optimal policy is one where at each state $s$ we choose the action that would result in the highest long term reward as parameterized by the $\\gamma$ value.  Luckily we have such a function that we can determine which action would result in the maximal long term reward namely $a_*(s) = $argmax$_{a'} q_*(s,a')$.  Thus the optimal policy is one where $\\pi(a_*(s)|s) \\rightarrow 1$.  If for each state an optimal policy can be determined, ie there is no ties in the argmax function then the policy is said to be deterministic $pi(s) = a$.  The optimal policy can be expressed as  \n",
    "\n",
    "$v_*(s) = $max$_a q_*(s,a)$\n",
    "\n",
    "$q_*(s,a) = r(s,a) + \\gamma \\sum_{s',r} P(s',r|s,a)$max$_{a'}[ q_*(s',a') ]$\n",
    "\n",
    "This recursive equation is essential to the concept of \"Q-learning.\" More will be said on how to compute or learn the $q_*(s,a)$ table in more complex environments later on. For now we seek to determine $q_*(s,a)$, how can this be done when we know $P(s',r|s,a)$?  Again the iterative method will allow us to find a convergent solution, as is done below using our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 21.89925005  20.80428755  16.86759588]\n",
      " [  1.12082922          nan   1.17982024]\n",
      " [         nan  53.87349498          nan]]\n",
      "pi_*(s) = [0 2 1]\n"
     ]
    }
   ],
   "source": [
    "# setting the q function q(s,a) to 0.0\n",
    "# shape=[s,a]\n",
    "Q = np.full((3,3),nan)\n",
    "for s,a in enumerate(possible_actions):\n",
    "    Q[s,a] = 0.0\n",
    "\n",
    "# not going to test for convergence\n",
    "for _ in range(MAX_ITERATIONS):\n",
    "    Q_old = Q.copy() #we've got to make a copy or we'll be updating s,a values asychronusly\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q[s,a] = r_sa[s,a] + 0.95*np.sum( [ P[s,a,s1]*np.max(Q_old[s1,possible_actions[s1]]) for s1 in range(3) ] )\n",
    "\n",
    "print(Q)\n",
    "# the nan will interfer with our argmax so lets set all nan to -infinity\n",
    "nan_idx = np.where(np.isnan(Q))\n",
    "Q[nan_idx] = -np.infty\n",
    "\n",
    "# find the optimal policy where we already know that there will be no ties\n",
    "# in the Q values per state s\n",
    "pi = np.argmax(Q,axis=1)\n",
    "print('pi_*(s) = '+str(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 General Value-Policy Improvement Loop\n",
    "\n",
    "There also exists other methods of finding optimal policies. Most of all algorithms that find the optimal polices are ones which follow the general value-policy improvement loop (GVPL).  Typically we will start with a random policy compute the value function $v_\\pi(s)$ then improve the policy by selecting transitions which would improve $v_\\pi(s)$ for example, when in state $s$ looking at all possible transition $v_\\pi(s')$ values, the optimal action is to move to the highest $v_\\pi(s')$ as that will nessecarily improve your current $v(s)$ value (this is known as a greedy algorithm and may be sub-optimal). By repeating this process we will end up in a convergent result where we will have the optimal policy an optimal value function (may be local or global optima). There are various methods of execution of this loop but it should be fairly straight forward if you've followed the previous sections.  \n",
    "\n",
    "What we're really interested in is MDPs where we do NOT know $P(s',r|s,a)$.  In these cases we have to use exploratory methods to try to empirically estimate $P(s',r|s,a)$ via Monte-Carlo sampling of sequences all while attempting to improve the policy as we gain new knowledge after each sequence. We explore these types of problems from here on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Unknown Transition Probablity MDP\n",
    "\n",
    "So how do we find the optimal policy when we do not know $P(s',r|s,a)$ but we can retrieve samples of the environment? In this case we use Markov chain monte carlo to simulate the environment, and use the large group of samples to make estimates of the exception values for the value function\n",
    "\n",
    "$v_\\pi(s) = \\mathbb{E}[G_t|S_t=s]$\n",
    "\n",
    "which can be estimated by $V(s)$ which is the empirical mean of all long term rewards $G_t$ received when state $s$ is encountered.  Following the GVPL we can then improve the policy until we find an optimal one so long as we're careful about not creating a bias in our policy that does not allow full exploration of the possible transitions for each state. The method we can use to update our estimate $V(s)$ is to use the fact that any mean can be updated via\n",
    "\n",
    "$\\mu_N = \\frac{1}{N} \\sum_{n=1}^N x_n = \\frac{1}{N} ( (N-1)\\mu_{N-1} + x_n ) = \\mu_{N-1} + \\frac{1}{N}(x_n - \\mu_{N-1})$\n",
    "\n",
    "Thus if we have a counter $N(s)$ which counts the number of times a state $s$ is visited in all sampled sequences referred to as episodes, we can update our estimate after each episode via \n",
    "\n",
    "$V(s) \\rightarrow  V(s) +\\frac{1}{N(s)}( G(s) - V(s))$\n",
    "\n",
    "where $G(s)$ is the current episode long term reward observed for state $s$. It is important to note that updating the policy after each estimate value function update creates a bias towards the states already seen.  It's possible that there is a better action that just by chance we have not observed enough times to create a reliable estimate of $V(s)$ because of this bias it is important to continue to take random actions to throughly explore the entire state space.  However if we always act randomly, we may never find the optimal policy as we'll spend too much time exploring paths that lead to less than optimal outcomes.  So which do we do? Do we act greedy and always follow the best policy after an episode is observed? Or do we act randomly to explore the state space more throughly?  This tension is known as the explore-exploit problem, and is generally solved by always enforcing that the policy is non-deterministic and does not change too rapidly from episode to episode, or via the epsilon greedy algorithm. \n",
    "\n",
    "The epsilon greedy algorithm states that when an action is to be made, choose the current policies best action with probability $1-\\epsilon$ and choose randomly from the possible actions with probability $\\epsilon$.  This ensures a good trade off between exploring and exploiting, especially when one cools the epsilon value such that at the end of your updating you are nearly always acting according to the policy and almost never randomly.  There is still a danger of falling into a local optima but that is running theme for many machine learning processes.  Lets apply the GVPL to an environment using the OpenAI gym package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading OpenAI GYM\n",
    "import gym\n",
    "\n",
    "# Here we're going to create an object\n",
    "# that will store all the knowledge our\n",
    "# agent gathers about its environment\n",
    "# including it's policy and its state\n",
    "# value function\n",
    "class FL_agent():\n",
    "    \n",
    "    def __init__(self,env,epsilon_greedy=0.0):\n",
    "        self.env = env\n",
    "        \n",
    "        self.S = env.observation_space.n\n",
    "        self.A = env.action_space.n\n",
    "        \n",
    "        #initialize if this is going to be a deterministic\n",
    "        #or probablistic policy (0.0 will use probablistic, any non 0.0 will be epsilon greedy)\n",
    "        self.epsilon_greedy = epsilon_greedy\n",
    "        \n",
    "        #initialize value function, policy\n",
    "        self.v = np.full(self.S,0.0)\n",
    "        self.pi = np.full((self.S,self.A),1./self.A)\n",
    "        \n",
    "        #initialize states visit counter\n",
    "        self.N = np.full(self.S,0)\n",
    "    \n",
    "    # define our agent's policy that will be used in the env\n",
    "    # note that we assume the actions are discrete\n",
    "    # and use an epsilon greedy algorithm and a probablistic\n",
    "    # policy which can generalize to a deterministic one\n",
    "    def policy(self,s):\n",
    "        if np.random.random() >= self.epsilon_greedy:\n",
    "            return int(np.random.choice(self.A,1,p = self.pi[s]))\n",
    "        else:\n",
    "            return np.random.randint(0,high=self.A)\n",
    "    \n",
    "    def value(self,s):\n",
    "        return self.v[s]\n",
    "    \n",
    "    # update the estimated state value function\n",
    "    # via the monte carlo method\n",
    "    def update_v_MC(self,states_sample,rewards_sample,discount_rate):\n",
    "        for t,s in enumerate(states_sample):\n",
    "            discount_reward = np.sum([discount_rate**t1*r for t1,r in enumerate(rewards_sample[t::])])\n",
    "            self.N[s] += 1\n",
    "            self.v[s] += (discount_reward-self.v[s])/self.N[s]\n",
    "\n",
    "    # update the agents policy given it's current\n",
    "    # estimation of the value state function\n",
    "    def update_policy(self,verbose=False):\n",
    "        nrow = self.env.env.nrow\n",
    "        ncol = self.env.env.ncol\n",
    "        \n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                action_v = np.array([])\n",
    "                for a in range(self.A):\n",
    "                    row1,col1 = row,col\n",
    "                    \n",
    "                    if a==0: # left\n",
    "                        col1 = max(col-1,0)\n",
    "                    elif a==1: # down\n",
    "                        row1 = min(row+1,nrow-1)\n",
    "                    elif a==2: # right\n",
    "                        col1 = min(col+1,ncol-1)\n",
    "                    elif a==3: # up\n",
    "                        row1 = max(row-1,0)\n",
    "                \n",
    "                    s = row*ncol + col\n",
    "                    s1 = row1*ncol + col1\n",
    "                                    \n",
    "                    letter = self.env.env.desc.reshape(-1,)[s1]\n",
    "                        \n",
    "                    if letter == 'G':\n",
    "                        r = 1.\n",
    "                    else:\n",
    "                        r = 0.\n",
    "                        \n",
    "                    # check if we fell in a hole or hit a wall\n",
    "                    if letter == 'H' or s == s1:\n",
    "                        act = 0.0\n",
    "                    else:\n",
    "                        act = self.v[s1]\n",
    "                    \n",
    "                    action_v = np.append(action_v,act)\n",
    "             \n",
    "                if verbose:\n",
    "                    print('s: ',s,' action_v: ', action_v)\n",
    "            \n",
    "                if self.epsilon_greedy > 0.0:\n",
    "                    one_hot_idx = np.argmax(action_v)\n",
    "                    self.pi[s] = np.full(self.A,0.0)\n",
    "                    self.pi[s,one_hot_idx] = 1.0\n",
    "                else:\n",
    "                    av_min = np.min(action_v)\n",
    "                    av_max = np.max(action_v)\n",
    "\n",
    "                    # handling the edge cases\n",
    "                    if av_min == av_max:\n",
    "                        if av_min == 0.0:\n",
    "                            action_v = action_v+1.0\n",
    "                        \n",
    "                        self.pi[s] = action_v/np.sum(action_v)\n",
    "                    else:\n",
    "                        # the typical case\n",
    "                        av_scaled = (action_v-av_min)/(av_max-av_min)+0.01\n",
    "                        av_scaled = av_scaled/np.sum(av_scaled)\n",
    "                        self.pi[s] = av_scaled\n",
    "                \n",
    "                    # another option is transforming the possible\n",
    "                    # transition actions via a softmax function\n",
    "                    # which is much less aggressive than the semi-greedy\n",
    "                    # exp_a = np.exp(action_v)\n",
    "                    # self.pi[s] = exp_a/np.sum(exp_a)\n",
    "                \n",
    "                \n",
    "\n",
    "# make a function to simulate an episode\n",
    "# that returns the sequence of transitions\n",
    "# and the sequence of instanteous rewards\n",
    "def sample_episode(env, policy):\n",
    "    s = env.reset()\n",
    "    states = []\n",
    "    rewards = []\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = policy(s)\n",
    "        s, R, done, _ = env.step(a)\n",
    "        states.append(s)\n",
    "        rewards.append(R)\n",
    "    \n",
    "    return(states, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the enviroment we're going to use\n",
    "# is called FrozenLake (FL) but the built\n",
    "# in OpenAI gym FL env has some random effects\n",
    "# where after an action it is not deterministic\n",
    "# that you arrive at the state you want\n",
    "# to make things simplier here I create an env\n",
    "# that is determinsitic to solve below\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78, # optimum = .8196\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our deterministic frozen lake env\n",
    "env_FL0 = gym.make('FrozenLakeNotSlippery-v0')\n",
    "\n",
    "# the objective is to go from S to G without falling\n",
    "# into a H state (hole state) the rewards are -1 for all\n",
    "# transitions thus the optimal policy is the shortest one\n",
    "\n",
    "# the environment looks like\n",
    "# SFFF  (S = start, F = frozen, H = hole, G = goal)\n",
    "# FHFH\n",
    "# FFFH\n",
    "# HFFG\n",
    "\n",
    "# possible actions at each state\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00818649,  0.00725732,  0.01371626,  0.00857475],\n",
       "       [ 0.00833559,  0.        ,  0.03502475,  0.        ],\n",
       "       [ 0.01970781,  0.06654163,  0.12279926,  0.        ],\n",
       "       [ 0.        ,  0.1613349 ,  0.42224334,  1.        ]])"
      ]
     },
     "execution_count": 997,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize our agent without \n",
    "# greedy epsilon policy\n",
    "agent_FL = FL_agent(env_FL0)\n",
    "\n",
    "# setting the max number of\n",
    "# episodes we're going to \n",
    "# experience and the discount \n",
    "# factor\n",
    "MAX_EPISODES = 5000\n",
    "y = 0.95\n",
    "\n",
    "# running the samples and updating\n",
    "# the state value functions based\n",
    "# on each episode\n",
    "for episode in range(MAX_EPISODES):\n",
    "    states,rewards = sample_episode(env_FL0, agent_FL.policy)\n",
    "    agent_FL.update_v_MC(states,rewards,y)  \n",
    "    \n",
    "# after all the episodes are run\n",
    "# reshape the state value function\n",
    "# into the shape of the FL env\n",
    "# so its easy to see state value functions\n",
    "agent_FL.v.reshape(-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('final epsilon: ', 0.056313514709472656)\n"
     ]
    }
   ],
   "source": [
    "# okay so it looks like it works...\n",
    "# lets run an epsilon greedy policy updating\n",
    "# method first acting totally random epsilon = 1.0\n",
    "# and we'll use a cooling acting more and more\n",
    "# determinstic over each policy update\n",
    "agent_FL = FL_agent(env_FL0,epsilon_greedy=1.0)\n",
    "\n",
    "MAX_EPISODES = 10000\n",
    "y = 1.0\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    states,rewards = sample_episode(env_FL0,agent_FL.policy)\n",
    "    agent_FL.update_v_MC(states,rewards,y)\n",
    "    \n",
    "    # after experiencing 1000 epsiodes\n",
    "    # update the internal policy using\n",
    "    # an epsilon greedy algo, and then\n",
    "    # cool the randomness of the policy\n",
    "    if (episode+1) % 1000 == 0:\n",
    "        agent_FL.update_policy()\n",
    "        agent_FL.epsilon_greedy = agent_FL.epsilon_greedy*0.75\n",
    "\n",
    "print('final epsilon: ',agent_FL.epsilon_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal determinisitic policy\n",
      "=============================\n",
      "[[1 2 1 0]\n",
      " [1 1 1 0]\n",
      " [2 1 1 1]\n",
      " [2 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# reminder of actions\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3\n",
    "\n",
    "# print out a visualization of the\n",
    "# policy and state value functions\n",
    "print('optimal determinisitic policy')\n",
    "print('=============================')\n",
    "print(np.argmax(agent_FL.pi,axis=1).reshape(-1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the optimal state value function\n",
    "optimal_deterministic_policy = agent_FL.pi\n",
    "\n",
    "agent = FL_agent(env_FL0,epsilon_greedy=0.0)\n",
    "\n",
    "MAX_EPISODES = 10000\n",
    "y = 0.9\n",
    "\n",
    "agent.pi = optimal_deterministic_policy\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    states,rewards = sample_episode(env_FL0,agent.policy)\n",
    "    agent.update_v_MC(states,rewards,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "optimal value function, gamma = 0.9\n",
      "=====================================\n",
      "[[ 0.       0.       0.       0.     ]\n",
      " [ 0.59049  0.       0.       0.     ]\n",
      " [ 0.6561   0.729    0.       0.     ]\n",
      " [ 0.       0.81     0.9      1.     ]]\n"
     ]
    }
   ],
   "source": [
    "print('\\noptimal value function, gamma = 0.9')\n",
    "print('=====================================')\n",
    "print agent.v.reshape(-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cool so we got that to work\n",
    "# the path always takes us to the\n",
    "# goal, now let's check how this \n",
    "# works when the environment is not \n",
    "# determinsitic and we use a probablistic\n",
    "# policy\n",
    "env_FL0 = gym.make('FrozenLake-v0')\n",
    "\n",
    "agent_FL = FL_agent(env_FL0)\n",
    "\n",
    "MAX_EPISODES = 10000\n",
    "y = 1.0\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    states,rewards = sample_episode(env_FL0,agent_FL.policy)\n",
    "    agent_FL.update_v_MC(states,rewards,y)\n",
    "    \n",
    "    # update via the semi-greedy algo\n",
    "    # every 500 epsiodes\n",
    "    if (episode+1) % 500 == 0:\n",
    "        agent_FL.update_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal probablistic policy\n",
      "=============================\n",
      "[[[ 0.00625406  0.63166007  0.35583181  0.00625406]\n",
      "  [ 0.39065514  0.00591597  0.59751292  0.00591597]\n",
      "  [ 0.16224953  0.68056815  0.15044402  0.0067383 ]\n",
      "  [ 0.97115385  0.00961538  0.00961538  0.00961538]]\n",
      "\n",
      " [[ 0.00737894  0.74527281  0.00737894  0.23996931]\n",
      "  [ 0.06506444  0.67495658  0.25329624  0.00668274]\n",
      "  [ 0.00836315  0.84467779  0.00836315  0.13859591]\n",
      "  [ 0.80589367  0.00797915  0.00797915  0.17814804]]\n",
      "\n",
      " [[ 0.00820791  0.00820791  0.82899903  0.15458515]\n",
      "  [ 0.12710973  0.48257902  0.38553324  0.00477801]\n",
      "  [ 0.22031965  0.67405318  0.00667379  0.09895337]\n",
      "  [ 0.19667292  0.7877285   0.00779929  0.00779929]]\n",
      "\n",
      " [[ 0.00771587  0.00771587  0.77930253  0.20526574]\n",
      "  [ 0.00735226  0.00735226  0.74257796  0.24271752]\n",
      "  [ 0.19790612  0.00630494  0.63679879  0.15899016]\n",
      "  [ 0.97115385  0.00961538  0.00961538  0.00961538]]]\n"
     ]
    }
   ],
   "source": [
    "# reminder actions\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3\n",
    "\n",
    "print('optimal probablistic policy')\n",
    "print('=============================')\n",
    "print(agent_FL.pi.reshape(-1,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_prob_policy = agent_FL.pi\n",
    "\n",
    "agent = FL_agent(env_FL0)\n",
    "\n",
    "MAX_EPISODES = 10000\n",
    "y = 0.9\n",
    "\n",
    "agent.pi = optim_prob_policy\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    states,rewards = sample_episode(env_FL0,agent.policy)\n",
    "    agent.update_v_MC(states,rewards,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "optimal value function, gamma = 0.9\n",
      "=====================================\n",
      "[[ 0.00966806  0.00503765  0.01453033  0.00830083]\n",
      " [ 0.01361418  0.          0.03646097  0.        ]\n",
      " [ 0.03724576  0.09824627  0.12673554  0.        ]\n",
      " [ 0.          0.19015276  0.38571966  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('\\noptimal value function, gamma = 0.9')\n",
    "print('=====================================')\n",
    "print agent.v.reshape(-1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temporal Difference Learning\n",
    "\n",
    "One disadvantage of the monte-carlo (MC) method is that entire episodes must be completed before we can update our value functions, and on top of that we have to wait for enough trails to find the expectation values for the policy updates.  For systems with huge state spaces these MC methods could take a very long time to complete even one episode!  Another issue with the MC method used above is that it assumes the environment is static, ie the environment doesn't evolve after each episode (formally this means that in a static enviornment $P(s',r|s,a)$ is fixed while it is not in non-static).  One obvious way to deal with non-static environments is to not update state value functions based on visit counts $N(s)$ but to use a moving or exponential average with a higher weight given to goals seen more recently such that our update equation becomes\n",
    "\n",
    "$V(s) \\rightarrow V(s) + \\alpha \\ (G(s) - V(s))$\n",
    "\n",
    "where $0 < \\alpha < 1$ so that terminal goals of recent episodes are more influential than past ones.  To address both the issue of non-stationary environments and potentially large state spaces we can use temporal differencing where we still estimate the state value function $V(s)$ but now we update during the episode via estimating the expected future return rather than computing the observed future return after a terminal state.  This means we will use MC methods to estimate\n",
    "\n",
    "$v_\\pi(s) = \\mathbb{E}[r(S_t) + \\gamma \\ v_\\pi(S_{t+1})|S_t=s]$\n",
    "\n",
    "this modification allows us to write our MC updating method as\n",
    "\n",
    "$V(s) \\rightarrow V(s) + \\alpha \\ ( r(s) + \\gamma \\ V(s') - V(s) )$\n",
    "\n",
    "where $s'$ is the observed transition state after action $\\pi(a|s)$ is sampled.  This method is known as a TD(0) or temporal differencing order 0 method, from a class of differencing methods known as TD$(\\lambda)$ methods.  It is import to note that in this method $V(s)$ is being updated by referencing itself, because of this, it is known as a bootstrapping method, and does not need to wait for a terminal state in order to update $V(S)$. Let's try this method out in the frozen lake environment to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FL_agent_TD():\n",
    "    \n",
    "    def __init__(self,env,epsilon_greedy=0.0, alpha=0.8):\n",
    "        self.env = env\n",
    "        \n",
    "        self.S = env.observation_space.n\n",
    "        self.A = env.action_space.n\n",
    "        \n",
    "        #initialize if this is going to be a deterministic\n",
    "        #or probablistic policy (0.0 will use probablistic, any non 0.0 will be epsilon greedy)\n",
    "        self.epsilon_greedy = epsilon_greedy\n",
    "        \n",
    "        #initialize value function, policy\n",
    "        self.v = np.full(self.S,0.0)\n",
    "        self.pi = np.full((self.S,self.A),1./self.A)\n",
    "        \n",
    "        #initialize states visit counter\n",
    "        self.N = np.full(self.S,0)\n",
    "        \n",
    "        # the non-stationary update rate\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def policy(self,s):\n",
    "        if np.random.random() >= self.epsilon_greedy:\n",
    "            return int(np.random.choice(self.A,1,p = self.pi[s]))\n",
    "        else:\n",
    "            return np.random.randint(0,high=self.A)\n",
    "    \n",
    "    def value(self,s):\n",
    "        return self.v[s]\n",
    "    \n",
    "    def update_v_MC(self,states_sample,rewards_sample,discount_rate):\n",
    "        for t,s in enumerate(states_sample):\n",
    "            discount_reward = np.sum([discount_rate**t1*r for t1,r in enumerate(rewards_sample[t::])])\n",
    "            self.N[s] += 1\n",
    "            self.v[s] += (discount_reward-self.v[s])/self.N[s]\n",
    "    \n",
    "    def update_v_TD0(self,s,R,s1,done,discount_rate):\n",
    "            self.v[s] += self.alpha*(R+discount_rate*self.v[s1]-self.v[s])\n",
    "\n",
    "    def update_policy(self,verbose=False):\n",
    "        nrow = self.env.env.nrow\n",
    "        ncol = self.env.env.ncol\n",
    "        \n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                action_v = np.array([])\n",
    "                for a in range(self.A):\n",
    "                    row1,col1 = row,col\n",
    "                    \n",
    "                    if a==0: # left\n",
    "                        col1 = max(col-1,0)\n",
    "                    elif a==1: # down\n",
    "                        row1 = min(row+1,nrow-1)\n",
    "                    elif a==2: # right\n",
    "                        col1 = min(col+1,ncol-1)\n",
    "                    elif a==3: # up\n",
    "                        row1 = max(row-1,0)\n",
    "                \n",
    "                    s = row*ncol + col\n",
    "                    s1 = row1*ncol + col1\n",
    "                                    \n",
    "                    letter = self.env.env.desc.reshape(-1,)[s1]\n",
    "                        \n",
    "                    if letter == 'G':\n",
    "                        r = 1.\n",
    "                    else:\n",
    "                        r = 0.\n",
    "                        \n",
    "                    # check if we fell in a hole or hit a wall\n",
    "                    if letter == 'H' or s == s1:\n",
    "                        act = 0.0\n",
    "                    else:\n",
    "                        act = r + self.v[s1]\n",
    "                    \n",
    "                    action_v = np.append(action_v,act)\n",
    "             \n",
    "                if verbose:\n",
    "                    print('s: ',s,' action_v: ', action_v)\n",
    "            \n",
    "                if self.epsilon_greedy > 0.0:\n",
    "                    one_hot_idx = np.argmax(action_v)\n",
    "                    self.pi[s] = np.full(self.A,0.0)\n",
    "                    self.pi[s,one_hot_idx] = 1.0\n",
    "                else:\n",
    "                    av_min = np.min(action_v)\n",
    "                    av_max = np.max(action_v)\n",
    "\n",
    "                    # handling the edge cases\n",
    "                    if av_min == av_max:\n",
    "                        if av_min == 0.0:\n",
    "                            action_v = action_v+1.0\n",
    "                        \n",
    "                        self.pi[s] = action_v/np.sum(action_v)\n",
    "                    else:\n",
    "                        # the typical case\n",
    "                        av_scaled = (action_v-av_min)/(av_max-av_min)+0.01\n",
    "                        av_scaled = av_scaled/np.sum(av_scaled)\n",
    "                        self.pi[s] = av_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning the optima via the TD(0) method\n",
    "env_FL = gym.make('FrozenLake-v0')\n",
    "\n",
    "agent_FL_TD = FL_agent_TD(env_FL,epsilon_greedy=1.0,alpha=0.7)\n",
    "\n",
    "MAX_EPISODES = 10000\n",
    "y = 0.9\n",
    "\n",
    "# now we update as we run the episode\n",
    "# as opposed to the MC method where \n",
    "# we run episodes then update\n",
    "for episode in range(MAX_EPISODES):\n",
    "    s = env_FL.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        a = agent_FL_TD.policy(s)\n",
    "        s1, R, done, _ = env_FL.step(a)\n",
    "        agent_FL_TD.update_v_TD0(s,R,s1,done,y)\n",
    "        s = s1\n",
    "        \n",
    "        if (episode+1) % 5000 == 0:\n",
    "            agent_FL_TD.update_policy()\n",
    "            agent_FL_TD.epsilon_greedy = agent_FL_TD.epsilon_greedy*0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder actions\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3\n",
    "\n",
    "agent_FL_TD.update_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.58161318e-03   3.04584394e-04   2.75551747e-03   9.11480634e-04]\n",
      " [  4.87408480e-04   0.00000000e+00   4.24986571e-02   0.00000000e+00]\n",
      " [  1.58670762e-02   7.76891082e-02   8.60393896e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   1.25522635e-01   9.76148452e-01   0.00000000e+00]]\n",
      "[[1 2 1 0]\n",
      " [1 1 1 0]\n",
      " [2 2 1 1]\n",
      " [2 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(agent_FL_TD.v.reshape(-1,4))\n",
    "print(np.argmax(agent_FL_TD.pi,axis=1).reshape(-1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Q-Learning\n",
    "\n",
    "All the methods discussed so far all make use of the GVPL to converge toward an optimal policy and state value function. When performing these iterations, we are continuously using the updated and improved policy to evaluate the the value function.  When we're using the actions of the policy to evaluate the optimality of the policy we say that this method is \"on-policy.\"  However there does exists a method where we do not need to update the policy at all to find the optimal policy! Methods that do not need to follow the optimal policy in order to find it are called \"off-policy.\"  The most useful off-policy method is that of Q-learning.  In Q-learning we're trying to learn through MC or TD methods the state-action value function $q_*(s,a)$ we've seen before in the fully known MDP section.  The optimal $q$ function was found to satisfy,\n",
    "\n",
    "$q_*(s,a) = r(s,a) + \\gamma \\sum_{s',r} P(s',r|s,a)$max$_{a'}[ q_*(s',a') ]$\n",
    "\n",
    "which will converge via iterative methods if $P(s',r|s,a)$ is known.  If we express this result in terms of expectation values\n",
    "\n",
    "$q_*(s,a) = \\mathbb{E}_\\pi[ r(s,a) + \\gamma \\ $max$_{a'} q_*(s',a') ] $\n",
    "\n",
    "we can combine the iterative method and MC or TD method to approximately solve $q_*(s,a)$ so long as we visit all $s,a$ pairs sufficiently using any method of sampling ie using ANY policy allows us to find $q_*(s,a)$ and thus the optimal policy!  Amazingly we can use any policy we like to estimate the true optimal policy.  This is like watching someone try every action repeately to try and accomplish a goal and noting which of those random actions eventually ended up in the best result.  After recording enough of the random events and their outcomes you can say which sequence of events is most likely to give you the best results.  \n",
    "\n",
    "To ensure we sufficiently sample the $s,a$ space, which potentially can get stuck in non-fruitful areas of the state space, or simpley be too big to explore everything, we sample the relevant space in a timely manner by using a policy derived from the $q$ function.  We can use epsilon greedy and use the $q$ values to update a policy (making it on-policy again) or we can encourage the currently used policy to explore unseen or rarely visited state action pairs by tracking the number of visits to each state action pairs $N(s,a)$ and using a curiosity function $f(n)$ increase the probability of selecting the less visited states.  In fact we can combine a curiosity function with the $q$ function so that as we saturate part of the state space important to the $q$ function we can promote searching unvisited or rarely visited $s,a$ combinations using an epsilon greedy algorithm along with the modification\n",
    "\n",
    "$q_{k+1}(s,a) \\rightarrow r(s,a) + \\gamma \\ $max$_{a'} f(q_k(s',a'),N(s',a'))$ \n",
    "\n",
    "$f(q,n)= q + \\frac{K}{1+n}$\n",
    "\n",
    "with this modification and using an epsilon greedy policy only after all the adjoining actions of state $s'$ are sufficiently sampled will $q(s,a)$ converge. Lets try our some Q learning on a simple known MDP that does not have a terminal state (thus TD(0) is a good choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(s',r|s,a) = P(s'|s,a)R(s'|s,a) transition probablities - pretend we don't know\n",
    "# this but can sample from it\n",
    "P = np.array([#shape=[s,a,s']\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],\n",
    "    [[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]]\n",
    "    ])\n",
    "\n",
    "# R(s'|s,a) rewards matrix - pretend we don't know this\n",
    "# but can sample it\n",
    "R = np.array([#shape=[s,a,s']\n",
    "    [[10., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n",
    "    [[0.0, 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -50.]],\n",
    "    [[nan, nan, nan], [40., 0.0, 0.0], [nan, nan, nan]]\n",
    "    ])\n",
    "\n",
    "\n",
    "# actions avaliable per state\n",
    "# shape=[s, action_space]\n",
    "possible_actions = [[0,1,2],[0,2],[1]]\n",
    "\n",
    "# our discount level\n",
    "y = 0.95\n",
    "# our TD learning rate -\n",
    "# level of influce from new\n",
    "# observations on Q[s,a] states\n",
    "alpha0 = 0.05\n",
    "\n",
    "# how long should our sample be?\n",
    "max_steps = 50000\n",
    "\n",
    "# starting state\n",
    "s = 0\n",
    "# initialize Q,policy,visits counter\n",
    "Q = np.full((3,3),-np.inf)\n",
    "pi = np.full((3,3),0.)\n",
    "N = np.full((3,3),0)\n",
    "\n",
    "# we're also going to track\n",
    "# variance of Q[s,a] states\n",
    "# to see how accurate our \n",
    "# optimal Q estimate is\n",
    "Q2 = np.full((3,3),-np.inf)\n",
    "Qstd = np.full((3,3),0.0)\n",
    "\n",
    "# initialization\n",
    "for s,a in enumerate(possible_actions):\n",
    "    Q[s,a] = 0.0\n",
    "    Q2[s,a] = 0.0\n",
    "    # random initial policy\n",
    "    pi[s,a] = len(possible_actions[s])**-1.\n",
    "\n",
    "# for visualizing the evolution of a Q[s,a] state\n",
    "q = list()\n",
    "\n",
    "# run the MDP\n",
    "for k in range(max_steps):\n",
    "    \n",
    "    # encourage balancing the sampling of \n",
    "    # state-action pairs inversely propto\n",
    "    # their visit count\n",
    "    if k > 1000:\n",
    "        for s in range(3):\n",
    "            va = possible_actions[s]\n",
    "            pi[s,va] = 1./N[s,va]/np.sum(1./N[s,va])\n",
    "    \n",
    "    # select the action\n",
    "    a =  np.random.choice(range(3),p = pi[s])\n",
    "    # determine the transition\n",
    "    s1 = np.random.choice(range(3), p = P[s,a])\n",
    "    \n",
    "    # increase the visits count\n",
    "    N[s,a] += 1\n",
    "    # append the visual state value\n",
    "    q.append(Q[1,2])\n",
    "    # compute the online variance of states\n",
    "    Q2[s,a] += ((R[s,a,s1] + y*np.max(Q[s1])) - Q[s,a])**2*alpha\n",
    "    # compute the TD(0) update with fixed alpha\n",
    "    Q[s,a] += (R[s,a,s1] + y*np.max(Q[s1]) - Q[s,a])*alpha\n",
    "    # update the current state before relooping\n",
    "    s = s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated Q_*[s,a] table\n",
      "========================\n",
      "[[ 22.02425757  21.3441346   19.26512742]\n",
      " [  2.6406052          -inf   4.26512792]\n",
      " [        -inf  53.64879869         -inf]]\n",
      "TD(0) evolution of s=1,a=2 state\n",
      "================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1c29b1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(\"estimated Q_*[s,a] table\")\n",
    "print(\"========================\")\n",
    "print(Q)\n",
    "\n",
    "print(\"TD(0) evolution of s=1,a=2 state\")\n",
    "print(\"================================\")\n",
    "plt.plot(q[0:2000],label=\"Q(s,a)=(1,2)\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Q_*(1,2)\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Table Estimated Exponential Avg Std Deviation\n",
      "===============================================\n",
      "[[   56.19578991     9.49458253    29.16436713]\n",
      " [    1.97969386     0.            27.05388686]\n",
      " [    0.          1213.77251657     0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q Table Estimated Exponential Avg Std Deviation\")\n",
    "print(\"===============================================\")\n",
    "for s in range(3):\n",
    "    for a in possible_actions[s]:\n",
    "        Qstd[s,a] = np.sqrt(Q2[s,a])\n",
    "\n",
    "print(Qstd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visits Counts - Should be Balanced\n",
      "==================================\n",
      "[[  211   214   231]\n",
      " [   98     0   119]\n",
      " [    0 49127     0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Visits Counts - Should be Balanced\")\n",
    "print(\"==================================\")\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visits do seem to be balanced, and we do happen to find the optimal deterministic policy by looking at $\\pi_*(s) =$  argmax$_{a}q_*(s,a)$ that we found when the MDP transitions were known explicitly, however we see that the values for $q_*(s,a)$ we computed above are estimates of the true $q_*(s,a)$ and by looking at the evolution of the $q(1,2)$ value we see it has quite a large range of possible values depending on when we stopped the simulation.  This is also evidenced in standard deviation table.  This simply reflects that ultimately the reward for going to state 3 isn't that large compared to the penalty of transitioning to it, in fact depending on run to run of this method you might find other estimates would not suggest the optimal move is to move from $s=2 \\rightarrow s=3$. So it is always important to view the variance of your estimates to get an idea of how accurate your estimate is.\n",
    "\n",
    "\n",
    "## 8. Large State Spaces and Approximate Q-Learning\n",
    "\n",
    "So far every method we've used has been tabular, ie we've constructed a tensor for $q,\\pi,v$ values and determined optimal policies by updating tables.  We either solved the Bellman equation via an iterative or inverting method, or used a monte carlo method to find the values for $s,a$ pairs.  However when we start to consider larger environments, these methods quickly become computationally intractable. The space of $s,a$ pairs grows too large and becomes too time consuming to explore throughly not to mention the storage required as well.  So how do we apply our concepts we've learned to large state spaces?  This yet another example of the curse of dimensionality and is a great place to use a function approximate for $q(s,a) \\approx q(s,a|\\theta)$ where $\\theta$ are tunable parameters. Through out the rest of this exploration of RL we'll focus on neural networks as our function approximator, though linear methods have been greatly explored.\n",
    "\n",
    "To get our heads around this method let's consider the cart pole problem.  Here, we have a cart that is free to move left or right while an upright pendulum is attached to its center.  The goal is to balance the pendulum upright for as long as possible with the possible actions to accelerate left or accelerate right. The states for the system is a 4-tuple of cart position along one dimension, cart velocity, angular deviation from upright for the pendulum, and angular velocity\n",
    "\n",
    "$s = (x,v,\\alpha,\\omega)$\n",
    "\n",
    "now we're dealing with a continuous state space. The rewards for the system is $r = 1$ for each transition regardless of action so long as $|x|<$edge_limit and $|\\alpha|<$tip_limit.  We will train a network choose an optimal action (accelerate left, accelerate right) given state $s$ as input.  For this task we'll use a fully connected 1 layer neural network to start with, where it will have the 4-tuple state as input and output $Q(s,a)$ for all actions possible (a 2-tuple in this case).  With one hidden layer our network is expressible as\n",
    "\n",
    "$Q(s,\\cdot|W,b) = W^{(2)}\\cdot f(W^{(1)}\\cdot s+b^{(1)}) + b^{(2)}$\n",
    "\n",
    "where $W^{(i)}$ are the connection weight matrices, $b^{(i)}$ layer bias vectors, and $f$ our activation function.  Typically we would train a network on labeled data of the form $s \\rightarrow Q_*(s,a)$ however we do not know the true $Q_*(s,a)$ we are trying to approximate but, however we can approximate the targets $Q_*(s,a)$ using the methods we developed in the previous sections! In order to find an estimate of the optimal $Q_*(s,a)$ function we can use $TD(0)$ learning to both run simulations of the environment and learn how to tune the estimator parameters towards the desired function, such that our estimator should approximately satisfy\n",
    "\n",
    "$Q(s,a|\\theta) \\approx  \\mathbb{E}_\\pi[ r(s,a) + \\gamma$ max$_{a'}( Q(s',a'|\\theta) )]$\n",
    "\n",
    "we can drive the estimator towards this approximate equality via TD sampling and using a loss function of the form\n",
    "\n",
    "$L(\\theta) = \\frac{1}{2} ( Q(s,a|\\theta) - [Q(s,a|\\theta) + \\alpha(r + \\gamma$ max$_{a'}( Q(s',a'|\\theta) ) - Q(s,a|\\theta) )] )^2$  \n",
    "\n",
    "$ = \\frac{\\alpha^2}{2} (r + \\gamma$ max$_{a'}( Q(s',a'|\\theta) ) - Q(s,a|\\theta) )^2$\n",
    "\n",
    "$L(\\theta) \\rightarrow \\frac{1}{2} [r + \\gamma$ max$_{a'}( Q(s',a'|\\theta) ) - Q(s,a|\\theta) ]^2$\n",
    "\n",
    "where the arrow indicates this loss function also admits the optimal $Q_*(s,a)$ approximation, though not technically an equality. Now we can use standard optimization methods to drive $\\theta$ towards the approximate equality.  Of course we want this to be minimized for all possible $(s,a,s',r)$-tuples over every possible episode sample $S_i$, ie we want to minimize\n",
    "\n",
    "min$_\\theta \\ L(\\theta) = $min$_\\theta \\ \\frac{1}{2} \\sum_{S_i} \\sum_{t_i=0}^{t_{i,N}} [ r_{i,t} +\\gamma \\ $max$_{a'} \\ Q(s'_{i,t},a_{i,t}|\\theta) -  Q(s_{i,t},a_{i,t}|\\theta) ]^2$\n",
    "\n",
    "where $S_i=\\left((s_0,a_0,s'_0,r_0)_i,(s_1,a_1,s'_1,r_1)_i,...,(s_T,a_T,s'_T,r_T)_i\\right)$ is the $i$-th sample of the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 How Does One Solve for $\\theta$?\n",
    "\n",
    "In principal one could simulate many sequences of an MDP, $\\Sigma = \\left(S_1,S_2,...,S_N\\right)$ with $S_i=\\left((s_0,a_0,s'_0,r_0)_i,(s_1,a_1,s'_1,r_1)_i,...,(s_T,a_T,s'_T,r_T)_i\\right)$ and from this large collection of samples use gradient descent methods to minimize the loss function\n",
    "\n",
    "$L(\\theta) = \\frac{1}{2} \\sum_{S_i} \\sum^{t_{i,N}}_{t_i=0} [r_{t,i} + \\gamma$ max$_{a'}( Q(s'_{t,i},a'|\\theta) ) - Q(s_{t,i},a_{t,i}|\\theta) ]^2$\n",
    "\n",
    "However in practice this does not tend to work very well, because although you're driving your approximation toward $Q_*(s,a)$ you have zero influence in the exporation method for $s,a$ pairs so for very large state spaces we will have to implement something closer to batch-stochastic gradient decent (bSGD) this way we can make a batch sample, and influence the sampling policy to explore more states of interest by optimizing \n",
    "\n",
    "$\\mathfrak{L}(\\theta,S_i) = \\frac{1}{2} \\sum^{t_{i,N}}_{t_i=0} [r_{t,i} + \\gamma$ max$_{a'}( Q(s'_{t,i},a'|\\theta) ) - Q(s_{t,i},a_{t,i}|\\theta) ]^2$\n",
    "\n",
    "Naturally, one might ask why not do SGD on every transition since the sum is commutive? In this case we're using transition based SGD optimization of\n",
    "\n",
    "$l(\\theta,s_{t,i},a_{t,i},s'_{t,i},r_{t,i}) = \\frac{1}{2} [r_{t,i} + \\gamma$ max$_{a'}( Q(s'_{t,i},a'|\\theta) ) - Q(s_{t,i},a_{t,i}|\\theta) ]^2$\n",
    "\n",
    "Naturally because we're using SGD we'll have to use very small step sizes in any of our gradient desecnt methods.  Let's see an example of this method on the simple FrozenLake-v0 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 99/8000 20-Win rate: 0.05 e: 0.494\n",
      "episode: 199/8000 20-Win rate: 0.0 e: 0.488\n",
      "episode: 299/8000 20-Win rate: 0.0 e: 0.481\n",
      "episode: 399/8000 20-Win rate: 0.0 e: 0.475\n",
      "episode: 499/8000 20-Win rate: 0.0 e: 0.469\n",
      "episode: 599/8000 20-Win rate: 0.15 e: 0.463\n",
      "episode: 699/8000 20-Win rate: 0.05 e: 0.456\n",
      "episode: 799/8000 20-Win rate: 0.05 e: 0.45\n",
      "episode: 899/8000 20-Win rate: 0.05 e: 0.444\n",
      "episode: 999/8000 20-Win rate: 0.05 e: 0.438\n",
      "episode: 1099/8000 20-Win rate: 0.1 e: 0.431\n",
      "episode: 1199/8000 20-Win rate: 0.0 e: 0.425\n",
      "episode: 1299/8000 20-Win rate: 0.0 e: 0.419\n",
      "episode: 1399/8000 20-Win rate: 0.05 e: 0.413\n",
      "episode: 1499/8000 20-Win rate: 0.1 e: 0.406\n",
      "episode: 1599/8000 20-Win rate: 0.1 e: 0.4\n",
      "episode: 1699/8000 20-Win rate: 0.1 e: 0.394\n",
      "episode: 1799/8000 20-Win rate: 0.05 e: 0.388\n",
      "episode: 1899/8000 20-Win rate: 0.1 e: 0.382\n",
      "episode: 1999/8000 20-Win rate: 0.05 e: 0.375\n",
      "episode: 2099/8000 20-Win rate: 0.1 e: 0.369\n",
      "episode: 2199/8000 20-Win rate: 0.1 e: 0.363\n",
      "episode: 2299/8000 20-Win rate: 0.0 e: 0.357\n",
      "episode: 2399/8000 20-Win rate: 0.1 e: 0.35\n",
      "episode: 2499/8000 20-Win rate: 0.0 e: 0.344\n",
      "episode: 2599/8000 20-Win rate: 0.15 e: 0.338\n",
      "episode: 2699/8000 20-Win rate: 0.1 e: 0.332\n",
      "episode: 2799/8000 20-Win rate: 0.1 e: 0.325\n",
      "episode: 2899/8000 20-Win rate: 0.15 e: 0.319\n",
      "episode: 2999/8000 20-Win rate: 0.2 e: 0.313\n",
      "episode: 3099/8000 20-Win rate: 0.1 e: 0.307\n",
      "episode: 3199/8000 20-Win rate: 0.35 e: 0.3\n",
      "episode: 3299/8000 20-Win rate: 0.1 e: 0.294\n",
      "episode: 3399/8000 20-Win rate: 0.05 e: 0.288\n",
      "episode: 3499/8000 20-Win rate: 0.2 e: 0.282\n",
      "episode: 3599/8000 20-Win rate: 0.0 e: 0.276\n",
      "episode: 3699/8000 20-Win rate: 0.2 e: 0.269\n",
      "episode: 3799/8000 20-Win rate: 0.1 e: 0.263\n",
      "episode: 3899/8000 20-Win rate: 0.15 e: 0.257\n",
      "episode: 3999/8000 20-Win rate: 0.15 e: 0.251\n",
      "episode: 4099/8000 20-Win rate: 0.15 e: 0.244\n",
      "episode: 4199/8000 20-Win rate: 0.1 e: 0.238\n",
      "episode: 4299/8000 20-Win rate: 0.2 e: 0.232\n",
      "episode: 4399/8000 20-Win rate: 0.1 e: 0.226\n",
      "episode: 4499/8000 20-Win rate: 0.0 e: 0.219\n",
      "episode: 4599/8000 20-Win rate: 0.15 e: 0.213\n",
      "episode: 4699/8000 20-Win rate: 0.2 e: 0.207\n",
      "episode: 4799/8000 20-Win rate: 0.2 e: 0.201\n",
      "episode: 4899/8000 20-Win rate: 0.0 e: 0.194\n",
      "episode: 4999/8000 20-Win rate: 0.15 e: 0.188\n",
      "episode: 5099/8000 20-Win rate: 0.15 e: 0.182\n",
      "episode: 5199/8000 20-Win rate: 0.4 e: 0.176\n",
      "episode: 5299/8000 20-Win rate: 0.25 e: 0.169\n",
      "episode: 5399/8000 20-Win rate: 0.15 e: 0.163\n",
      "episode: 5499/8000 20-Win rate: 0.2 e: 0.157\n",
      "episode: 5599/8000 20-Win rate: 0.25 e: 0.151\n",
      "episode: 5699/8000 20-Win rate: 0.2 e: 0.145\n",
      "episode: 5799/8000 20-Win rate: 0.4 e: 0.138\n",
      "episode: 5899/8000 20-Win rate: 0.25 e: 0.132\n",
      "episode: 5999/8000 20-Win rate: 0.5 e: 0.126\n",
      "episode: 6099/8000 20-Win rate: 0.25 e: 0.12\n",
      "episode: 6199/8000 20-Win rate: 0.45 e: 0.113\n",
      "episode: 6299/8000 20-Win rate: 0.2 e: 0.107\n",
      "episode: 6399/8000 20-Win rate: 0.3 e: 0.101\n",
      "episode: 6499/8000 20-Win rate: 0.35 e: 0.0946\n",
      "episode: 6599/8000 20-Win rate: 0.3 e: 0.0884\n",
      "episode: 6699/8000 20-Win rate: 0.25 e: 0.0821\n",
      "episode: 6799/8000 20-Win rate: 0.3 e: 0.0759\n",
      "episode: 6899/8000 20-Win rate: 0.2 e: 0.0697\n",
      "episode: 6999/8000 20-Win rate: 0.4 e: 0.0634\n",
      "episode: 7099/8000 20-Win rate: 0.3 e: 0.0572\n",
      "episode: 7199/8000 20-Win rate: 0.35 e: 0.051\n",
      "episode: 7299/8000 20-Win rate: 0.5 e: 0.0447\n",
      "episode: 7399/8000 20-Win rate: 0.45 e: 0.0385\n",
      "episode: 7499/8000 20-Win rate: 0.6 e: 0.0322\n",
      "episode: 7599/8000 20-Win rate: 0.6 e: 0.026\n",
      "episode: 7699/8000 20-Win rate: 0.45 e: 0.0198\n",
      "episode: 7799/8000 20-Win rate: 0.3 e: 0.0135\n",
      "episode: 7899/8000 20-Win rate: 0.3 e: 0.0073\n",
      "episode: 7999/8000 20-Win rate: 0.65 e: 0.00106\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "# using Keras to build our function approximator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# we're going to use a linear approximator for our Q(s,a) function\n",
    "Q_approx = Sequential()\n",
    "# use small random initialization of weights\n",
    "rand = RandomUniform(minval=0.0, maxval=0.01, seed=None)\n",
    "# dont include a bias, ie our model is Q(s,a) = s.W\n",
    "Q_approx.add(Dense(4,input_shape=(16,),use_bias=False,activation=\"linear\",kernel_initializer=rand))\n",
    "# use gradient decent with step size fixed to 0.125\n",
    "sgd = SGD(lr=0.125, momentum=0.0, decay=0.0, nesterov=False)\n",
    "# using mean square error for optimization\n",
    "Q_approx.compile(loss=\"mse\",optimizer=sgd)\n",
    "\n",
    "# discount reward factor\n",
    "gamma = 0.9\n",
    "# how random should we act when looking\n",
    "# for optimal actions at the start?\n",
    "epsilon_0 = 0.5\n",
    "# how random should we act towards the\n",
    "# end of all our simulations\n",
    "epsilon_F = 0.001\n",
    "# how many episodes should we sample?\n",
    "max_episodes = 8000\n",
    "# how many steps should we let an episode\n",
    "# take before cutting it off?\n",
    "max_T = 100\n",
    "\n",
    "# we will use one hot encoding for our state\n",
    "# vectors s = 1x16 vector with all zeros expect 1\n",
    "# this is a collection of One Hot vectors\n",
    "# where row 1 is the OHE for state 1\n",
    "# row 2 is the OHE for state 2 and so on...\n",
    "OHE_states = np.identity(16)\n",
    "\n",
    "#goals list for eval\n",
    "G_list = []\n",
    "T_list = []\n",
    "\n",
    "epsilon = epsilon_0\n",
    "\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# start sampling the enviornment\n",
    "for epi in range(max_episodes):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    while t < max_T:\n",
    "        t += 1\n",
    "        #it wants something with same (1,16)\n",
    "        #so add a lil more idx to give it \n",
    "        #the right shape\n",
    "        OHE_s = OHE_states[s:s+1]\n",
    "        #output shape is input rows X 4\n",
    "        # in this case its (1,4)\n",
    "        Q_s = Q_approx.predict(OHE_s)\n",
    "\n",
    "        #epsilon greedy action selection\n",
    "        if np.random.random() <= epsilon:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            a = np.argmax(Q_s[0])\n",
    "        \n",
    "        s1,r,d,_ = env.step(a)\n",
    "        \n",
    "        OHE_s1 = OHE_states[s1:s1+1]\n",
    "        Q_s1 = Q_approx.predict(OHE_s1)\n",
    "        max_a_Q_s1 = np.amax(Q_s1)\n",
    "        target_Q_s = Q_s\n",
    "        target_Q_s[0,a] = r + gamma*max_a_Q_s1\n",
    "        \n",
    "        # take a SGD step\n",
    "        Q_approx.fit(OHE_s,target_Q_s,epochs=1,verbose=0)\n",
    "        \n",
    "        s = s1\n",
    "        if d:\n",
    "            break\n",
    "    \n",
    "    # cool the chances of random action\n",
    "    epsilon = epsilon_0*(1.-epi/float(max_episodes)) + epsilon_F*(epi/float(max_episodes))\n",
    "    G_list.append(r)\n",
    "    T_list.append(t)\n",
    "    \n",
    "    # print status updates\n",
    "    if (epi+1)%100 == 0:\n",
    "        win_rate = np.sum(np.array(G_list[-20:]) > 0)/20.\n",
    "        print(\"episode: {}/{} 20-Win rate: {} e: {:.3}\".format(epi,max_episodes,win_rate,epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05688397  0.05396281  0.05403933  0.05401371]\n",
      " [ 0.03457741  0.04427769  0.04806162  0.05383124]\n",
      " [ 0.06173662  0.06222605  0.06925987  0.0584262 ]\n",
      " [ 0.03954676  0.04209962  0.03549012  0.05320517]\n",
      " [ 0.07550281  0.06198359  0.0616239   0.05638246]\n",
      " [ 0.00701722  0.00950799  0.00325666  0.00644062]\n",
      " [ 0.10785128  0.07275303  0.07295133  0.03393855]\n",
      " [ 0.00558792  0.0051061   0.00347821  0.00661942]\n",
      " [ 0.09020524  0.09174997  0.0761812   0.10470066]\n",
      " [ 0.16740301  0.24670726  0.19302788  0.12820294]\n",
      " [ 0.32204688  0.20449397  0.19328339  0.09719525]\n",
      " [ 0.00408718  0.00632348  0.00725746  0.00141312]\n",
      " [ 0.00674742  0.0009746   0.00157362  0.00152149]\n",
      " [ 0.21137629  0.30591199  0.40691161  0.29795143]\n",
      " [ 0.43196863  0.52086741  0.69714808  0.50644255]\n",
      " [ 0.00568838  0.00930547  0.00394364  0.00590765]]\n"
     ]
    }
   ],
   "source": [
    "# Reminder of the actions in this enviornment\n",
    "#LEFT = 0\n",
    "#DOWN = 1\n",
    "#RIGHT = 2\n",
    "#UP = 3\n",
    "\n",
    "# Map of the frozen lake\n",
    "#\"SFFF\"\n",
    "#\"FHFH\"\n",
    "#\"FFFH\"\n",
    "#\"HFFG\"\n",
    " \n",
    "# taking a look at the Q(s,.|W) ~ s.W model values \n",
    "print(Q_approx.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1c5dea10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "# Lets look at how the moving average of 10 eps \n",
    "# evolves using epsilon greedy actions and our\n",
    "# function approximator to Q(s,a)\n",
    "result = running_mean(G_list,10)\n",
    "plt.plot(result,label=\"10 episode goal moving average\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1d7d7dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets also examen the 10 epsiode moving average \n",
    "# number of steps which are taken before a terminal\n",
    "# state is encountered\n",
    "result = running_mean(T_list,10)\n",
    "plt.plot(result,label=\"10 episode total steps mave\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear approximation model  we used above,\n",
    "\n",
    "$Q(s,a|W) = (s \\cdot W)_a$ \n",
    "\n",
    "does pretty well at the end of 8000 simulations. It tends to find the goal even after taking a non-optimal number of steps (see the moving average of number of steps is increasing to above 20 at the same time the moving average goal is increasing).  However if you change any of the parameters in the model you will find that even after many steps there is no guarantee of convergence! If we didn't know this would work, we might spend ages looking for the right parameters and potentially conclude that this doesn't work if the approximator never did well. How do can we improve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Experience Replay and Target Networks\n",
    "\n",
    "In the above algorithm you might have noticed we were using SGD to find the optimal approximator to the true $Q_*(s,a)$.  However to correctly use SGD you have to randomly sample from all possible points which are summed over in the loss function, ie randomly sample from $(s_{i,t},a_{i,t},r_{i,t},s'_{i,t})$ (see appendix A for a reminder on SGD) however the way we currently run the algorithm we do not sample independent experiences at each step, instead we're sampling highly correlated points.  In order to reduce the correlations between observations and obtain better convergence, one can use a method called experience replay.  In experience replay we store a history of observed $(s,a,r,s')$-tuples (experiences), when we update our function approximation we sample uniformly from all recorded observations this way our SGD is unlikely to have highly correlated sequences.\n",
    "\n",
    "\n",
    "Another method that is used to improve convergence is to use target and prediction approximate functions to approximate the optimal $Q_*(s,a)$ function.  In the target and prediction function method we take optimization steps over a replay sample on a loss function given by\n",
    "\n",
    "$L(\\theta) = \\frac{1}{2} \\sum_{replay_i} \\sum^{t_{i,N}}_{t_i=0} [r_{t,i} + \\gamma$ max$_{a'}( Q(s'_{t,i},a'|\\theta_t) ) - Q(s_{t,i},a_{t,i}|\\theta_p) ]^2$\n",
    "\n",
    "where $Q(s,a|\\theta_t)$, the target function is an identical function approximator to $Q(s,a|\\theta_p)$ but with different parameters $(\\theta_t,\\theta_p)$.  The optimization steps are done with respect to $\\theta_p$ while $\\theta_a$ is held fixed. If we slowly or periodically adjust $\\theta_t$ toward $\\theta_p$ values the SGD method tends to have better convergence behavior as the target our approximator is driving towards doesn't have as strong dependence on itself.  Other computational tricks have also been found in recent years such as replacing the term \n",
    "\n",
    "max$_{a'} Q(s'_{t,i},a'|\\theta_t) \\rightarrow Q(s'_{t,i},\\ $argmax$_{a'}[ Q(s'_{t,i},a'|\\theta_p)] \\ |\\theta_t)$\n",
    "\n",
    "known as the double-DQN (DDQN) technique, one of many other numerical tricks employed today. Now that we're in possession of all these tricks and knowledge we're finally in a position to attempt to model the CartPole environment with approximate Q-learning techniques via a neural network as is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# going to use an Adam optimizer this time\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# implementing an experience replay pool class\n",
    "# we have to put some limit on the size\n",
    "# as the memory is not inifinite, so we ask\n",
    "# that we only store the most recent set of \n",
    "# experiences as the most revelent\n",
    "class replay():\n",
    "    def __init__(self,max_pool_size=2000):\n",
    "        self.maxpool = max_pool_size\n",
    "        self.memory_size = 0\n",
    "        self.memory = deque(maxlen=self.maxpool)\n",
    "    \n",
    "    def add(self,event):\n",
    "        self.memory.append(event)\n",
    "        self.memory_size = len(self.memory)\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        sample_size = min(batch_size,self.memory_size)    \n",
    "        return random.sample(self.memory,sample_size)\n",
    "\n",
    "# implementing a DQN agent based on fully connected net\n",
    "class DQNAgent(object):\n",
    "    def __init__(self,env,hidden_dims=[[4,\"relu\"]]):\n",
    "        # MDP problem variables\n",
    "        self.gamma = 0.99\n",
    "        # temp is used as epsilon or tempurature in\n",
    "        # non deterministic policies with boltzmann\n",
    "        # style policies\n",
    "        self.Temp = 1.0\n",
    "        self.Temp_min = 0.001\n",
    "        self.cooling_rate = 0.995\n",
    "        #when we use SGD methods we have to use very small \n",
    "        #steps which means really long run times\n",
    "        self.lr = 0.005\n",
    "        self.targ_rate = 0.001\n",
    "        \n",
    "        # Env variables\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.action_list = range(self.action_size)\n",
    "        \n",
    "        # Q Approximator variables\n",
    "        self.hidden_dims = np.array(hidden_dims)\n",
    "        \n",
    "        #build fully connected (fc) networks\n",
    "        #based on hidden dims variable\n",
    "        self.Qtarg = self.build_fc_network()\n",
    "        self.Qpred = self.build_fc_network()\n",
    "        \n",
    "        # Replay batch params\n",
    "        self.batch_size = 32\n",
    "        self.replay_pool = replay(max_pool_size=2000)\n",
    "    \n",
    "    # fully connected network builder\n",
    "    def build_fc_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(int(self.hidden_dims[0,0]), input_dim=self.state_size, activation=self.hidden_dims[0,1]))\n",
    "        \n",
    "        if len(self.hidden_dims) > 1:\n",
    "            for hdim,fn in self.hidden_dims[1:]:\n",
    "                model.add(Dense(int(hdim), activation=fn))\n",
    "                \n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        #im fixing the loss function to the Mean Square Errors (mse)\n",
    "        #and to use an Adam optimizer\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=self.lr))\n",
    "        return model\n",
    "    \n",
    "    def clone_target(self):\n",
    "        #copy weights from predictions to target\n",
    "        self.target.set_weights(self.predict.get_weights())\n",
    "    \n",
    "    def iterate_target_weights(self):\n",
    "        # slowly use a learning rate type update\n",
    "        # for the target network weights to progress\n",
    "        # towards those of the prediction network\n",
    "        W_update = []\n",
    "        for w_t,w_p in zip(self.Qtarg.get_weights(),self.Qpred.get_weights()):\n",
    "            temp = w_t + self.targ_rate*(w_p - w_t)\n",
    "            W_update.append(temp)\n",
    "            \n",
    "        self.Qtarg.set_weights(W_update)\n",
    "    \n",
    "    # for non-deterministic policy selection\n",
    "    def softmax_action(self,s):\n",
    "        s_in = s.reshape(1,self.state_size)\n",
    "        Qp_s = self.Qpred.predict(s_in)[0]\n",
    "        # somtimes this has overflow issues \n",
    "        # using exp(number) so to stop this\n",
    "        # we scale all the values so that\n",
    "        # mathematically the result is the same\n",
    "        # but the risk of overflow isnt there\n",
    "        Qp_s = Qp_s - np.max(Qp_s)\n",
    "        # clipping limit\n",
    "        Qp_s[Qp_s < -10.] = -10\n",
    "        softmax = np.exp(Qp_s/self.Temp)\n",
    "        softmax = softmax/np.sum(softmax)\n",
    "        return np.random.choice(self.action_list,p=softmax)\n",
    "    \n",
    "    # for epsilon greedy action selection\n",
    "    def ep_greedy_action(self,s):        \n",
    "        if np.random.random() < self.Temp:\n",
    "            a = np.random.choice(self.action_list)\n",
    "        else:\n",
    "            s_in = s.reshape(1,self.state_size)\n",
    "            Qp_s = self.Qpred.predict(s_in)[0]\n",
    "            a = np.argmax(Qp_s)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    # experience based SGD method\n",
    "    def fit_SGD_replay(self,use_DDQN=False):        \n",
    "        batch = self.replay_pool.sample(batch_size=self.batch_size)\n",
    "        #generate batch inputs and targets\n",
    "        for s,a,r,s1,d in batch:\n",
    "            s_in = s.reshape(1,self.state_size)\n",
    "            MSE_target = self.Qpred.predict(s_in)\n",
    "            if d:\n",
    "                MSE_target[0,a] = r\n",
    "            else:\n",
    "                s1_in = s1.reshape(1,self.state_size)\n",
    "                if use_DDQN:\n",
    "                    targ = self.Qtarg.predict(s1_in)[0]\n",
    "                    a1 = np.argmax(self.Qpred.predict(s1_in)[0])\n",
    "                    targ = targ[a1]\n",
    "                else:\n",
    "                    targ = np.max(self.Qtarg.predict(s1_in)[0])\n",
    "                \n",
    "                MSE_target[0,a] = r + self.gamma*targ\n",
    "            \n",
    "            self.Qpred.fit(s_in,MSE_target,epochs=1,verbose=0)\n",
    "        \n",
    "        if self.Temp > self.Temp_min:\n",
    "            self.Temp *= self.cooling_rate\n",
    "    \n",
    "    # mini batch experience replay SGD method\n",
    "    def fit_bSGD_replay(self,use_DDQN=False):        \n",
    "        batch = self.replay_pool.sample(batch_size=self.batch_size)\n",
    "        s_in_vect = []\n",
    "        t_vect = []\n",
    "        #generate batch inputs and targets\n",
    "        for s,a,r,s1,d in batch:\n",
    "            s_in = s.reshape(1,self.state_size)\n",
    "            MSE_target = self.Qpred.predict(s_in)\n",
    "            if d:\n",
    "                MSE_target[0,a] = r\n",
    "            else:\n",
    "                s1_in = s1.reshape(1,self.state_size)\n",
    "                if use_DDQN:\n",
    "                    targ = self.Qtarg.predict(s1_in)[0]\n",
    "                    a1 = np.argmax(self.Qpred.predict(s1_in)[0])\n",
    "                    targ = targ[a1]\n",
    "                else:\n",
    "                    targ = np.max(self.Qtarg.predict(s1_in)[0])\n",
    "                \n",
    "                MSE_target[0,a] = r + self.gamma*targ\n",
    "            \n",
    "            s_in_vect.append(s_in[0])\n",
    "            t_vect.append(MSE_target[0])\n",
    "        \n",
    "        s_in_vect = np.array(s_in_vect)\n",
    "        t_vect = np.array(t_vect)\n",
    "        self.Qpred.fit(s_in_vect,t_vect,epochs=1,verbose=0)\n",
    "        \n",
    "        if self.Temp > self.Temp_min:\n",
    "            self.Temp *= self.cooling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "('Initial Temp: ', 1.0)\n",
      "('Minimum Temp: ', 0.1)\n",
      "('Exponential Cooling Rate: ', 0.999671113367499)\n",
      "('Starting Env Simulation -- Max Episodes: ', 7000)\n",
      "--------------------------------\n",
      "episode: 500/7000, score: 39, Temp:0.877\n",
      "episode: 1000/7000, score: 10, Temp:0.744\n",
      "episode: 1500/7000, score: 8, Temp:0.631\n",
      "episode: 2000/7000, score: 15, Temp:0.535\n",
      "episode: 2500/7000, score: 9, Temp:0.454\n",
      "episode: 3000/7000, score: 38, Temp:0.385\n",
      "episode: 3500/7000, score: 12, Temp:0.327\n",
      "episode: 4000/7000, score: 13, Temp:0.277\n",
      "episode: 4500/7000, score: 71, Temp:0.235\n",
      "episode: 5000/7000, score: 87, Temp:0.2\n",
      "episode: 5500/7000, score: 106, Temp:0.169\n",
      "episode: 6000/7000, score: 97, Temp:0.144\n",
      "episode: 6500/7000, score: 171, Temp:0.122\n",
      "episode: 7000/7000, score: 137, Temp:0.103\n",
      "completed episodes\n",
      "('mean reward: ', 61.304285714285712)\n",
      "('mean stddev: ', 67.822375017845147)\n",
      "('last 10: ', [167, 141, 256, 243, 118, 111, 114, 127, 105])\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# run time parameters\n",
    "max_episodes = 7000\n",
    "print_int = 500\n",
    "max_episode_length = 1000\n",
    "# dont start any training until\n",
    "# a few samples have been taken \n",
    "# to avoid any bias towards\n",
    "# certain actions\n",
    "pre_train_size = 100\n",
    "R_record = []\n",
    "# goign to use 1 hidden layer with 16 units with relu activations\n",
    "agent = DQNAgent(env=env,hidden_dims=[[16,\"relu\"]])\n",
    "\n",
    "# start totally random\n",
    "agent.Temp = 1.0\n",
    "# and cool to pretty deterministic\n",
    "agent.Temp_min = 0.1\n",
    "agent.cooling_rate = agent.Temp_min**(1./max_episodes)\n",
    "print('Initial Temp: ', agent.Temp)\n",
    "print('Minimum Temp: ', agent.Temp_min)\n",
    "print('Exponential Cooling Rate: ', agent.cooling_rate)\n",
    "print('Starting Env Simulation -- Max Episodes: ', max_episodes)\n",
    "print('--------------------------------')\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    s = env.reset()\n",
    "    for t in range(max_episode_length):\n",
    "        a = agent.ep_greedy_action(s)\n",
    "        #a = agent.softmax_action(s)\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        # adding a penalty for going too close to the walls which is\n",
    "        # at +-2.4 distance units\n",
    "        r = r + 0.00*(1./(s1[0]-2.4)**5 - 1./(s1[0]+2.4)**5+2./(2.4**5))\n",
    "        agent.replay_pool.add((s,a,r,s1,d))    \n",
    "        s = s1\n",
    "        if d:\n",
    "            agent.iterate_target_weights()\n",
    "            break\n",
    "    \n",
    "    # lets avoid making any changes until the pool\n",
    "    # fills with a few samples\n",
    "    if episode > pre_train_size:\n",
    "        agent.fit_bSGD_replay(use_DDQN=False)\n",
    "    \n",
    "    if episode%10 == 0:\n",
    "        R_record.append(t)\n",
    "        \n",
    "    if (episode+1) % print_int == 0:\n",
    "        print(\"episode: {}/{}, score: {}, Temp:{:.3}\".format(episode+1,max_episodes,t,agent.Temp))\n",
    "    \n",
    "print(\"completed episodes\")\n",
    "print(\"mean reward: \", np.mean(R_record))\n",
    "print(\"mean stddev: \", np.std(R_record))\n",
    "print(\"last 10: \", R_record[-10:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122e7d150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the rewards every 10 epsiodes and the moving average\n",
    "# of the rewards as the simulation runs\n",
    "plt.plot(range(len(R_record)),R_record,color='blue',label='episode reward')\n",
    "\n",
    "moving_ave = running_mean(R_record,50)\n",
    "mv_x = [(x+50) for x in range(len(moving_ave))]\n",
    "\n",
    "plt.plot(mv_x,moving_ave,color='cyan',label='50 episode moving average')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "Finally we're at the point where we are able to create an agent who can perform a fairly complex task with no information about its environment but solely by gathering it's own data and knowing what it's goal is, can learn to perform optimally.  Reinforcement learning continues to be an active area of research today, and many groups are attempting to find improvements to these deep neural network training algorithms. There are many more methods to explore such as policy gradients, and the very successful actor-critic models. Many blogs and articles can be found on this subject today and I encourage you to look for information on these topics.  The OpenAI gym contains many more environments to explore and problems to solve and I encourage you to explore them.  Good luck and thank you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "Let's remind ourselves how the stochastic gradient descent method works, and why it works.  When trying to optimize a scaler function given by a sum of terms, $f(\\theta) = \\sum_{n=1}^N f_n(\\theta)$ the gradient decent algorithm for minimizing $f(\\theta)$ is accomplished via repeated use of\n",
    "\n",
    "$\\theta_{k+1} = \\theta_k - \\alpha \\ \\partial_\\theta f(\\theta_k)$\n",
    "\n",
    "which results in\n",
    "\n",
    "$f(\\theta_{k+1}) \\approx f(\\theta_k) - \\alpha \\ \\left( \\partial_\\theta f(\\theta_k) \\right)^2 < f(\\theta_k)$ so long as\n",
    "$\\alpha \\ \\left(\\partial_\\theta f(\\theta_k) \\right)^2 \\ll f(\\theta_k)$\n",
    "\n",
    "therefore so long as $\\alpha$ remains sufficiently small repeated updating of the $\\theta$ parameters always moves you to a smaller value of $f(\\theta)$. In stochastic gradient decent (SGD) our iteration rule is modified by \n",
    "\n",
    "$\\theta_{k+1} = \\theta_k - \\alpha' \\ \\partial_\\theta f_j(\\theta_k)$\n",
    "\n",
    "where $j$ runs from $(1,N)$ successively.  The question is after many applications of this iteration policy, do we minimize $f(\\theta)$? Let's consider $f(\\theta_M)$ vs. $f(\\theta_0)$ by evaluating $\\theta_M$ for some $0 < M$. \n",
    "\n",
    "$\\theta_1 = \\theta_0 - \\alpha' \\ \\partial_\\theta f_{1}(\\theta_0)$\n",
    "\n",
    "$\\theta_2 = \\theta_1 - \\alpha' \\ \\partial_\\theta f_{2}(\\theta_1)$\n",
    "\n",
    "$=\\theta_0 - \\alpha' \\ \\partial_\\theta f_{1}(\\theta_0) - \\alpha' \\ \\partial_\\theta f_{2}(\\theta_0) +O(\\alpha'^2)$\n",
    "\n",
    "...\n",
    "\n",
    "$\\theta_M = \\theta_0 - \\alpha' \\left( \\partial_\\theta f_{1}(\\theta_0)+\\partial_\\theta f_{2}(\\theta_0)+...+\\partial_\\theta f_{M}(\\theta_0)\\right) + O(\\alpha^2)$\n",
    "\n",
    "...\n",
    "\n",
    "$\\theta_N=\\theta_0 - \\alpha' \\ \\partial_\\theta \\left( \\sum_{n=1}^N f_n(\\theta_0) \\right) + O(\\alpha'^2)$\n",
    "\n",
    "$=\\theta_0 - \\alpha' \\ \\partial_\\theta f(\\theta_0) + O(\\alpha^2)$\n",
    "\n",
    "We see that $\\theta_N$ is approximately the same as evaluating a single step of the full batch gradient decent so long as $\\alpha' \\ll \\alpha$.  Thus running this iteration algorithm for many more times than the size of the data set will allow you to find the same optima as the full batch results.  However, one might ask is it necessary that I use every $f_j(\\theta)$ to accomplish this same goal?  The answer is no, this is obvious when one considers when doing the full batch gradient descent method, the step for $\\theta_k$ is given by\n",
    "\n",
    "$\\delta \\theta_k = -\\alpha \\ \\partial_\\theta \\left( \\sum_j f_j(\\theta_k) \\right)$\n",
    "\n",
    "$ = -\\alpha \\ \\left(\\partial_\\theta f_1(\\theta_k) + ... + \\partial_\\theta f_N(\\theta_k) \\right)$\n",
    "\n",
    "The expansion suggests that each function $f_j(\\theta_k)$ \"votes\" on how to adjust the parameter $\\theta_k$ all of the votes are summed and the step is taken.  Consider if many of $f_j(\\theta)$ are already near their minimum they would vote not to move very much at all while a single vote that is far from its particular optima will try to adjust the parameters, the act of this gradient descent is to balance these votes so that every function gets as close to their optima as possible.  In this view rather than sum up all votes, if we only sum a small batch of votes that are taken UNIFORMLY and making smaller adjustments ie $\\alpha' < \\alpha$ then we will drive towards the global optima (the balance of votes) if we repeat this iterative procedure.  Of course the size of the step will depend on the size of the sample of votes, thus as you use fewer and fewer votes to decide the parameter adjustment you must use smaller and smaller steps to ensure convergence.\n",
    "\n",
    "Note that the uniform sampling of votes is cruicual to letting this method work.  If we took samples from highly correlated functions and took many steps based on these functions then we would drive ourselves away from the optima and cause us to never converge on an optimal parameter as we would repeatly be moving towards a group's votes then another group of correlated functions votes and a cycle would be established."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
